---
title: "NDC_Analysis"
author: "sid"
date: "8/25/2022"
output: html_document
---

```{r,echo=FALSE}
library(pdftools)
library(dplyr)
library(tm)
library(readr)
library(stringr)
library(tidytext)
library(ggplot2)
library(cowplot)
library(tidyr)
library(scales)
library(forcats)
library(ggraph)
library(igraph)
library(cowplot)
library(openxlsx)
```

Reading and cleaning the NDC data, and cleaning it.

Reading and cleaning the NDCs for Developed countries:

Reading US NDC.

```{r, include=FALSE}


#reading the US NDC------------
txt_US <- pdf_text("United States NDC April 21 2021 Final.pdf") %>% 
  read_lines()

str(txt_US)

#converting the character vector into a tibble
txt_USdf <- tibble(line=1:778, text=txt_US)

#adding NAs to the blank rows
txt_USdf[txt_USdf==""] <- NA

#removing rows with NAs
txt_USdf <- txt_USdf %>% 
  na.omit() 

txt_USdf <- txt_USdf[-8,]


#adding a column with index number
txt_USdf$linenumber <- 1:nrow(txt_USdf)

#moving the index number column to the start
txt_USdf <- txt_USdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_USdf$text <- str_to_lower(txt_USdf$text)

#addin a column
txt_USdf$country <- "US"

#adding grouping column
txt_USdf$grouping <- "Developed"

```

Reading the NDC of France, Italy and Germany as EU. As all the Eu countries submitted a combined NDC under the EU banner:

```{r, include=FALSE}
#reading the NDC------------
txt_EU <- pdf_text("EU_NDC_Submission_December 2020.pdf") %>% 
  read_lines()

str(txt_EU)

#converting the character vector into a tibble
txt_EUdf <- tibble(line=1:774, text=txt_EU)


#adding NAs to the blank rows
txt_EUdf[txt_EUdf==""] <- NA

#removing rows with NAs
txt_EUdf <- txt_EUdf %>% 
  na.omit() 

#removing rows with numbers
txt_EUdf <- txt_EUdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_EUdf$linenumber <- 1:nrow(txt_EUdf)

#moving the index number column to the start
txt_EUdf <- txt_EUdf %>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_EUdf$text <- str_to_lower(txt_EUdf$text)

#addin a column
txt_EUdf$country <- "EU"

#adding grouping column
txt_EUdf$grouping <- "Developed"

```

Reading UK's NDC.

```{r, include=FALSE}

#reading the NDC------------
txt_UK <- pdf_text("UK Nationally Determined Contribution.pdf") %>% 
  read_lines()

str(txt_UK)

#converting the character vector into a tibble
txt_UKdf <- tibble(line=1:1019, text=txt_UK)

#removing the copyright rows
txt_UKdf <- txt_UKdf[-c(5:16),] 

#adding NAs to the blank rows
txt_UKdf[txt_UKdf==""] <- NA

#removing rows with NAs
txt_UKdf <- txt_UKdf %>% 
  na.omit() 

#removing rows with numbers
txt_UKdf  <- txt_UKdf %>% 
  filter(!grepl("\\d+$", text))


#adding a column with index number
txt_UKdf$linenumber <- 1:nrow(txt_UKdf)

#moving the index number column to the start
txt_UKdf <- txt_UKdf %>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_UKdf$text <- str_to_lower(txt_UKdf$text)

#addin a column
txt_UKdf$country <- "UK"

#adding grouping column
txt_UKdf$grouping <- "Developed"

```

Reading Australia's NDC.

```{r, include=FALSE}

#reading the NDC------------
txt_AUS <- pdf_text("Australias NDC June 2022 Update (3).pdf") %>% 
  read_lines()

str(txt_AUS)

#converting the character vector into a tibble
txt_AUSdf <- tibble(line=1:508, text=txt_AUS)

#removing the copyright rows
txt_AUSdf <- txt_AUSdf[-c(4:48),] 

#adding NAs to the blank rows
txt_AUSdf[txt_AUSdf==""] <- NA

#removing rows with NAs
txt_AUSdf <- txt_AUSdf %>% 
  na.omit() 

#removing rows with just numbers
txt_AUSdf  <- txt_AUSdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_AUSdf$linenumber <- 1:nrow(txt_AUSdf)

#moving the index number column to the start
txt_AUSdf <- txt_AUSdf %>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_AUSdf$text <- str_to_lower(txt_AUSdf$text)

#addin a column
txt_AUSdf$country <- "Australia"

#adding grouping column
txt_AUSdf$grouping <- "Developed"

```


Reading South Korea's NDC

```{r, include=FALSE}
#reading the NDC------------
txt_KOR <- pdf_text("211223_The Republic of Korea's NDC.pdf") %>% 
  read_lines()

str(txt_KOR)

#converting the character vector into a tibble
txt_KORdf <- tibble(line=1:980, text=txt_KOR)

#adding NAs to the blank rows
txt_KORdf[txt_KORdf==""] <- NA

#removing rows with NAs
txt_KORdf <- txt_KORdf %>% 
  na.omit() 

#removing rows with just numbers
txt_KORdf <- txt_KORdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_KORdf$linenumber <- 1:nrow(txt_KORdf)

#moving the index number column to the start
txt_KORdf <- txt_KORdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_KORdf$text <- str_to_lower(txt_KORdf$text)

#addin a column
txt_KORdf$country <- "South Korea"

#adding grouping column
txt_KORdf$grouping <- "Developed"

```


Reading Canada's NDC.

```{r, include=FALSE}
#reading the NDC------------
txt_CAN <- pdf_text("Canada's Enhanced NDC Submission1_FINAL EN.pdf") %>% 
  read_lines()

str(txt_CAN)

#converting the character vector into a tibble
txt_CANdf <- tibble(line=1:1810, text=txt_CAN)

#adding NAs to the blank rows
txt_CANdf[txt_CANdf==""] <- NA

#removing rows with NAs
txt_CANdf <- txt_CANdf %>% 
  na.omit() 

#removing rows with just numbers
txt_CANdf <- txt_CANdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_CANdf$linenumber <- 1:nrow(txt_CANdf)

#moving the index number column to the start
txt_CANdf <- txt_CANdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_CANdf$text <- str_to_lower(txt_CANdf$text)

#addin a column
txt_CANdf$country <- "Canada"

#adding grouping column
txt_CANdf$grouping <- "Developed"

```

Reading Japan's NDC.

```{r, include=FALSE}
#reading the NDC------------
txt_JAP <- pdf_text("JAPAN_FIRST NDC (UPDATED SUBMISSION).pdf") %>% 
  read_lines()

str(txt_JAP)

#converting the character vector into a tibble
txt_JAPdf <- tibble(line=1:404, text=txt_JAP)

#adding NAs to the blank rows
txt_JAPdf[txt_JAPdf==""] <- NA

#removing rows with NAs
txt_JAPdf <- txt_JAPdf %>% 
  na.omit() 

#removing rows with just numbers
txt_JAPdf <- txt_JAPdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_JAPdf$linenumber <- 1:nrow(txt_JAPdf)

#moving the index number column to the start
txt_JAPdf <- txt_JAPdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_JAPdf$text <- str_to_lower(txt_JAPdf$text)

#addin a column
txt_JAPdf$country <- "Japan"

#adding grouping column
txt_JAPdf$grouping <- "Developed"

```

Reading developing/G12 countries NDCs:

Reading India's NDC.

```{r, echo=FALSE, warning=FALSE, include=FALSE}
#reading pdf and converting the text one line per row (India's NDC)
txt_india <- pdf_text("INDIA INDC TO UNFCCC.pdf") %>% 
  read_lines()

str(txt_india)

#converting the character vector into a tibble
txt_indiadf <- tibble(line=1:1399, text=txt_india)

#adding NAs to the blank rows
txt_indiadf[txt_indiadf==""] <- NA

#removing rows with NAs
txt_indiadf <- txt_indiadf %>% 
  na.omit() 

#removing rows with Sanskrit language
txt_indiadf <- txt_indiadf[-c(3:6),] 

#removing rows with page number
txt_indiadf <- txt_indiadf %>% 
  filter(!grepl("Page", text)) 

#adding a column with index number
txt_indiadf$linenumber <- 1:nrow(txt_indiadf)

#moving the index number column to the start
txt_indiadf <- txt_indiadf %>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_indiadf$text <- str_to_lower(txt_indiadf$text)

#addin a column
txt_indiadf$country <- "India"

#adding grouping column
txt_indiadf$grouping <- "Developing/G12"

```


Reading Bangladesh NDC.

```{r, include=FALSE}
#reading the NDC------------
txt_BAN <- pdf_text("Bangladesh_NDC_submission_20210826revised (11).pdf") %>% 
  read_lines()

str(txt_BAN)

#converting the character vector into a tibble
txt_BANdf <- tibble(line=1:1488, text=txt_BAN)

#drop not necessary rows
txt_BANdf <- txt_BANdf[-c(1:17),] 

#adding NAs to the blank rows
txt_BANdf[txt_BANdf==""] <- NA

#removing rows with NAs
txt_BANdf <- txt_BANdf %>% 
  na.omit() 

#removing rows with just numbers
txt_BANdf<- txt_BANdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_BANdf$linenumber <- 1:nrow(txt_BANdf)

#moving the index number column to the start
txt_BANdf <- txt_BANdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_BANdf$text <- str_to_lower(txt_BANdf$text)

#addin a column
txt_BANdf$country <- "Bangladesh"

#adding grouping column
txt_BANdf$grouping <- "Developing/G12"

```

Reading Brazil's NDC.

```{r, include=FALSE}
#reading the NDC------------
txt_BRA <- pdf_text("Brazil_Updated - First NDC -  FINAL - PDF.pdf") %>% 
  read_lines()

str(txt_BRA)

#converting the character vector into a tibble
txt_BRAdf <- tibble(line=1:462, text=txt_BRA)

#removing some rows
txt_BRAdf <- txt_BRAdf[-c(1:7),] 

#adding NAs to the blank rows
txt_BRAdf[txt_BRAdf==""] <- NA

#removing rows with NAs
txt_BRAdf <- txt_BRAdf %>% 
  na.omit() 

#removing rows with just numbers
txt_BRAdf<- txt_BRAdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_BRAdf$linenumber <- 1:nrow(txt_BRAdf)

#moving the index number column to the start
txt_BRAdf <- txt_BRAdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_BRAdf$text <- str_to_lower(txt_BRAdf$text)

#addin a column
txt_BRAdf$country <- "Brazil"

#adding grouping column
txt_BRAdf$grouping <- "Developing/G12"

```


Reading Egyptian INDC.

```{r, include=FALSE}
#reading the NDC------------
txt_EGP <- pdf_text("Egyptian INDC.pdf") %>% 
  read_lines()

str(txt_EGP)

#converting the character vector into a tibble
txt_EGPdf <- tibble(line=1:563, text=txt_EGP)

#removing some rows
txt_EGPdf <- txt_EGPdf[-c(1:8),] 

#adding NAs to the blank rows
txt_EGPdf [txt_EGPdf ==""] <- NA

#removing rows with NAs
txt_EGPdf  <- txt_EGPdf  %>% 
  na.omit()

#removing rows with just numbers
txt_EGPdf<- txt_EGPdf %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_EGPdf$linenumber <- 1:nrow(txt_EGPdf)

#moving the index number column to the start
txt_EGPdf<- txt_EGPdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_EGPdf$text <- str_to_lower(txt_EGPdf$text)

#addin a column
txt_EGPdf$country <- "Egypt"

#adding grouping column
txt_EGPdf$grouping <- "Developing/G12"

```

Reading Saudi Arabia's NDC.

```{r, include=FALSE}
#reading the NDC------------
txt_SUD <- pdf_text("Saudi_arabia_NDC 2021.pdf") %>%
  read_lines()

str(txt_SUD)

#converting the character vector into a tibble
txt_SUDdf <- tibble(line=1:524, text=txt_SUD)

#removing some rows
txt_SUDdf <- txt_SUDdf[-c(1:15),]

#adding NAs to the blank rows
txt_SUDdf [txt_SUDdf ==""] <- NA

#removing rows with NAs
txt_SUDdf   <- txt_SUDdf  %>%
  na.omit()

#removing rows with just numbers
txt_SUDdf<- txt_SUDdf %>%
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_SUDdf$linenumber <- 1:nrow(txt_SUDdf)

#moving the index number column to the start
txt_SUDdf<- txt_SUDdf%>%
  select(-line) %>%
  relocate(linenumber, .before = text)

#converting every letter to lower case
txt_SUDdf$text <- str_to_lower(txt_SUDdf$text)

#addin a column
txt_SUDdf$country <- "Saudi Arabia"

#adding grouping column
txt_SUDdf$grouping <- "Developed"


```

Reading South Africa's NDC.


```{r, include=FALSE}
#reading the NDC------------
txt_SA <- pdf_text("South Africa updated first NDC September 2021.pdf") %>% 
  read_lines()

str(txt_SA)

#converting the character vector into a tibble
txt_SAdf <- tibble(line=1:1271, text=txt_SA)

#Removing some rows
txt_SAdf <- txt_SAdf[-c(1:45),] 

#adding NAs to the blank rows
txt_SAdf [txt_SAdf ==""] <- NA

#removing rows with NAs
txt_SAdf   <- txt_SAdf  %>% 
  na.omit()

#removing rows with just numbers
txt_SAdf <- txt_SAdf  %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_SAdf$linenumber <- 1:nrow(txt_SAdf)

#moving the index number column to the start
txt_SAdf<- txt_SAdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_SAdf$text <- str_to_lower(txt_SAdf$text)

#addin a column
txt_SAdf$country <- "South Africa"

#adding grouping column
txt_SAdf$grouping <- "Developing/G12"


```

Reading Turkey NDC.

```{r, include=FALSE}
#reading the NDC------------
txt_TUR <- pdf_text("The_INDC_of_TURKEY_v.15.19.30.pdf") %>% 
  read_lines()

str(txt_TUR)

#converting the character vector into a tibble
txt_TURdf <- tibble(line=1:224, text=txt_TUR)

#removing some rows
txt_TURdf <- txt_TURdf[-c(1:5),] 

#adding NAs to the blank rows
txt_TURdf [txt_TURdf  ==""] <- NA

#removing rows with NAs
txt_TURdf <- txt_TURdf   %>% 
  na.omit()

#removing rows with just numbers
txt_TURdf <- txt_TURdf  %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_TURdf$linenumber <- 1:nrow(txt_TURdf)

#moving the index number column to the start
txt_TURdf<- txt_TURdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_TURdf$text <- str_to_lower(txt_TURdf$text)

#addin a column
txt_TURdf$country <- "Turkey"

#adding grouping column
txt_TURdf$grouping <- "Developing/G12"

```

Reading Indonesia's NDC.

```{r, include=FALSE}
#reading the NDC------------
txt_IDN <- pdf_text("Updated NDC Indonesia 2021 - corrected version.pdf") %>% 
  read_lines()

str(txt_IDN)

#converting the character vector into a tibble
txt_IDNdf <- tibble(line=1:1650, text=txt_IDN)

#removing some rows
txt_IDNdf <- txt_IDNdf[-c(1:147),]

#adding NAs to the blank rows
txt_IDNdf [txt_IDNdf  ==""] <- NA

#removing rows with NAs
txt_IDNdf <- txt_IDNdf %>% 
  na.omit()

#removing rows with just numbers
txt_IDNdf <- txt_IDNdf  %>% 
  filter(!grepl("\\d+$", text))

#adding a column with index number
txt_IDNdf$linenumber <- 1:nrow(txt_IDNdf)

#moving the index number column to the start
txt_IDNdf<- txt_IDNdf%>% 
  select(-line) %>% 
  relocate(linenumber, .before = text) 

#converting every letter to lower case
txt_IDNdf$text <- str_to_lower(txt_IDNdf$text)

#addin a column
txt_IDNdf$country <- "Indonesia"

#adding grouping column
txt_IDNdf$grouping <- "Developing/G12"

```


Combining all the NDC data frames for G8 and G20 nations.

```{r, include=FALSE}

combined_NDC <- rbind(txt_USdf, txt_EUdf, txt_UKdf, txt_AUSdf, txt_KORdf, txt_CANdf, txt_JAPdf, txt_SUDdf,
      txt_indiadf, txt_BANdf, txt_BRAdf, txt_EGPdf,  txt_SAdf, txt_TURdf, txt_IDNdf)



```

Conducting analysis using one-gram.

Tokenizing the corpus (one word) and drawing the top 30 words for the country.
```{r}
remove_words <- c("canada", "uk", "eu", "india", "european", "uk’s", "canada’s", "uk’s", "africa", "korea", "indonesia", "africa’s", "bangladesh", "australia’s", "scotland", "brazil", "saudi", "australia", "canadian", "african", "south",
                  "egypt", "korean", "turkey", "www.legislation.gov.uk", "egyptian", "canadians", "indonesia’s", 
                  "ireland", "korea’s", "india’s", "mw", "ha", "km", "gw", "eu's", "srn", "inr", "ccc", "alberta", "wales", "welsh", "scottish", "australian", "japan's", "inuit", "www.gov.scot", "japan", "kingdom",
                  "scotland's", "gn", "scp", "ec", "pcf", "www.gov.uk", "gba", "gpg", "https", "net", "million", "ii", "iii", "vii", "viii", "________________________",  "___________________________________", "abc", "aa", "sa", "sa’s")


combined_NDC %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  filter(!str_detect(word, "\\d")) %>% 
  filter(!word%in%remove_words) %>% 
  mutate(word=gsub("\\www\\w*", NA, word)) %>% 
  na.omit() %>% 
  count(country, word, sort=T) %>% 
  group_by(country) %>%
  slice_max(n, n=10) %>%
  ungroup() %>% 
  filter(!(country=="Brazil" & word%in%c("including", "nationally", "applicable"))) %>% 
  mutate(country=as.factor(country),
         word=reorder_within(word, n, country)) %>% 
  ggplot(aes(n, word,n))+
  geom_col(fill="steelblue")+
  facet_wrap(~country, scales="free")+
  scale_y_reordered() +
  theme(axis.text = element_text(face="bold"),
        strip.text=element_text(face="bold"))+
  labs(y=NULL)

#ggsave("count_country.jpg", dpi = 300, height = 8, width = 12)


```
Above chart shows the the term frequency of top 10 words in each countries NDC. Note that we have removed some words which do not provide any interesting information like names of countries, etc.

Some observations from the term-frequency of words in the NDCs of different countries:

* Different countries have different sector focus and different emphasis on different sectors. We assume that higher the frequency of mention of certain sector, high is the focus on that sector. Energy was the focus of NDCs for most countries in the g8 and G20-G8. 

* Countries like India, Indonesia, Egypt, Saudi Arabia focused on Development. 
* Majority of the developing countries like Brazil, Egypt, Indonesia, and South Africa had high frequency of adaptation in their NDCs showing the increased focus of developing countries (i.e. G20-G8) countries on adaptation.
* India, Egypt and Saudi Arabia also had strong focus on water sector in their NDCs. 
* None of the G8 countries had adaptation as their top 10 frequency words.
* The focus of G8 countries is on mitigation as countries like Canada, US, South Korea, and Japan had GHG or carbon as their top 10 high frequency words. 

Tokenizing the corpus (one word) and drawing the top 30 words for the grouping

```{r}
combined_NDC %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  filter(!str_detect(word, "\\d")) %>% 
  filter(!word%in%remove_words) %>% 
  count(grouping, word, sort=T) %>% 
  group_by(grouping) %>%
  slice_max(n, n=40) %>%
  ungroup() %>% 
  mutate(grouping=as.factor(grouping),
         word=reorder_within(word, n, grouping)) %>% 
  ggplot(aes(n, word,n, fill=grouping))+
  geom_col()+
  facet_wrap(~grouping, scales="free")+
  theme(legend.position = "none")+
  scale_y_reordered() +
  labs(y=NULL)
```
In the above chart we looked at the frequency of words in the combined G8 and G20-G8 countries. Based on the chart, we make the following observations:

* There is clear difference in the focus of G8 and G20-G8 countries based on the analysis of their NDCs.
* We looked at the top 40 high frequency words in both G8 and G20-G8 countries and found that G20-G8 countries have strong focus on development, and adaptation.
* G20-G8 countries also focus on sectors like water, waste and technology. These countries also mention words like capacity and support which means that these countries also focus on capacity building and need for support to achieve their goals.
* Sustainability and efficiency also gets mentioned a lot of times in G20-G8 countries NDCs. 
* On contrary, G8 countries do not have strong focus on any of these areas. They have high focus on GHG emissions, and reduction. Although both G8 and G20-G8 have strong focus on energy sector. 

**Calculating at tf-idf.**

The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites. 

```{r}

combined_NDC %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  filter(!word%in%remove_words) %>% 
  filter(!str_detect(word, "\\d")) %>% 
  count(grouping, word, sort=T) %>% 
  bind_tf_idf(word, grouping, n) %>% 
  #arrange(desc(tf_idf)) %>% 
  group_by(grouping) %>%
  slice_max(tf_idf, n = 40) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = grouping)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~grouping, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```


In the above chart we looked at which words occur more frequently in G8 and G20-G8.

However, it does not tell us easily which words occur more frequently in one set of documents that the other set. Foloowing scatter plot helps in this comparison between the two sets of documents. 
```{r}


token_combined <- combined_NDC %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  filter(!word%in%remove_words) %>% 
  filter(!str_detect(word, "\\d")) %>% 
  count(grouping, word, sort=T)

#token_combined %>% View()

frequency_combined <- token_combined %>%
    group_by(grouping) %>% 
  mutate(proportion=n/sum(n)) %>% 
   select(-n) %>% 
  pivot_wider(names_from = grouping, values_from = proportion) %>% 
   pivot_longer(`Developed`,
               names_to = "country", values_to = "proportion")

#frequency_combined %>% View()


word_subset <- c("capacity", "finance", "development","neutrality", "water",  "disaster",
                 "parliament", "regulation",  "neutral", "legislation")

freq_subset <- frequency_combined %>% 
  filter(word%in%word_subset)

ggplot(frequency_combined, aes(x = proportion, y = `Developing/G12`,
                      color=abs(`Developing/G12`-proportion)))+
  geom_abline(color="gray40", lty=2)+
  geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3)+
  geom_point(data = freq_subset, color="red", size=2.5, width=0.3, height=0.3)+
  geom_text(aes(label=word), check_overlap = TRUE, vjust=1.5)+
  geom_text(data = freq_subset, aes(label=word), color="red",vjust=1.5)+
  scale_x_log10(labels=percent_format())+
  scale_y_log10(labels=percent_format())+
  scale_color_gradient(limits=c(0,0.001),
                       low = "darkslategray4", high = "gray75")+
  facet_wrap(~country)+
  theme_light()+
  theme(legend.position = "none",
        strip.text=element_text(face="bold"))+
  labs(y="Developing", x=NULL)

ggsave("proportion_1gram.jpg", dpi = 300, height = 7, width = 10)


```



correlation between words in different texts. 

```{r}
#correlation between words in different texts
cor.test(data=frequency_combined[frequency_combined$country=="Developed",], 
         ~ proportion + `Developing/G12`)
```

#### Tokenizing by n-grams

In the previous section we analyzed data using 1-gram. Here we will use n-grams.
```{r}
combined_NDC %>% 
  unnest_tokens(bigram, text, token = "ngrams", n=2) %>% 
  count(grouping,bigram, sort=T)
```


```{r}


combined_bigram <- combined_NDC %>% 
  unnest_tokens(bigram, text, token = "ngrams", n=2)


bigram_filtered <- combined_bigram %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!str_detect(word1, "\\d")) %>% 
  filter(!str_detect(word2, "\\d")) %>% 
    filter(!word1%in%remove_words) %>% 
    filter(!word2%in%remove_words) 

bigram_filtered %>% 
  count(grouping, word1, word2, sort = TRUE)

```
```{r}
bigram_united <- bigram_filtered %>% 
  unite(bigram, word1, word2, sep=" ")

bigram_united %>% 
  count(grouping, bigram, sort=T)
```
```{r}
bigram_united%>% 
  count(grouping, bigram, sort=T) %>% 
  group_by(grouping) %>%
  slice_max(n, n=30) %>%
  ungroup() %>% 
  mutate(grouping=as.factor(grouping),
         bigram=reorder_within(bigram, n, grouping)) %>%
  ggplot(aes(n, bigram,n, fill=grouping))+
  geom_col()+
  facet_wrap(~grouping, scales="free")+
  theme(axis.text = element_text(face="bold"),
        strip.text=element_text(face="bold"),
        legend.position = "none")+
  scale_y_reordered() +
  labs(y=NULL)

#ggsave("bigram_text.png", dpi = 300, height = 8, width = 10)
 
 #write.xlsx(xx, "bigrams.xlsx")
```

tf-idf for the bigrams.

```{r}
bigram_united%>% 
  count(grouping, bigram, sort=T) %>% 
  bind_tf_idf(bigram, grouping, n) %>% 
  #arrange(desc(tf_idf)) %>% 
  group_by(grouping) %>%
  slice_max(tf_idf, n = 40) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = grouping)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~grouping, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

Creating the scatter plot for bigrams.

```{r}

frequency_bigram <- bigram_united %>% 
  count(grouping, bigram, sort=T) %>% 
  group_by(grouping) %>% 
  mutate(proportion=n/sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = grouping, values_from = proportion) %>% 
  pivot_longer(`Developed`,
               names_to = "country", values_to = "proportion")


ggplot(frequency_bigram, aes(x = proportion, y = `Developing/G12`,
                      color=abs(`Developing/G12`-proportion)))+
  geom_abline(color="gray40", lty=2)+
  geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3)+
  geom_text(aes(label=bigram), check_overlap = TRUE, vjust=1.5)+
  scale_x_log10(labels=percent_format())+
  scale_y_log10(labels=percent_format())+
  scale_color_gradient(limits=c(0,0.001),
                       low = "darkslategray4", high = "gray75")+
  facet_wrap(~country)+
  theme_light()+
  theme(legend.position = "none")+
  labs(y="Developing/G12", x=NULL)

```

Lets visualize network biagram.
Here we have filtered any bigram which has frequency greater than 10. However, the point here is not to look at frequency but which are the cneter words which spawn many connections.
```{r}
bigram_count <- bigram_filtered %>% 
  count(grouping, word1, word2, sort = TRUE)

bigram_count_g8 <- bigram_count %>% 
  filter(grouping=="Developed") %>% 
  select(-grouping)

bigram_count_g20_g8 <- bigram_count %>% 
  filter(grouping=="Developing/G12") %>% 
   select(-grouping)

#filter for relatively common combinations for g8
bigram_graph_g8 <- bigram_count_g8 %>% 
  filter(n>15) %>% 
  graph_from_data_frame()

#filter for relatively common combinations for g20-g8
bigram_graph_g20_g8 <- bigram_count_g20_g8 %>% 
  filter(n>15) %>% 
  graph_from_data_frame()


bigram_graph_g8
bigram_graph_g20_g8


```

```{r}
set.seed(2017)


ggraph(bigram_graph_g20_g8, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```
Lets look at G20-G8 network plot.


```{r}
set.seed(2020)

#creating network diagram for G20-G8 NDCs
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

network_g20_g8 <- ggraph(bigram_graph_g20_g8, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

network_g20_g8

ggsave("network_g20_g8.png", dpi = 300, height = 8, width = 14, bg = "white")


```





Lets look at G8 network plot.
```{r}
set.seed(2020)

#creating network diagram for G8 NDCs
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

network_g8 <- ggraph(bigram_graph_g8, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

network_g8

ggsave("network_g8.png", dpi = 300, height = 8, width = 12, bg = "white")
```
```{r}
plot_grid(network_g8, network_g20_g8, labels = c('Developed', 'Developing/G12'), label_size = 16, scale = 0.95, label_colour = "Black")


#ggsave("network_combined.png", dpi = 300, height = 8, width = 16, bg = "white")
```


Lets see the results for tri-grams.

```{r}
remove_words <- c("canada", "uk", "eu", "india", "european", "uk’s", "canada’s", "uk’s", "africa", "korea", "indonesia", "africa’s", "bangladesh", "australia’s", "scotland", "brazil", "saudi", "australia", "canadian", "african", "south",
                  "egypt", "korean", "turkey", "www.legislation.gov.uk", "egyptian", "canadians", "indonesia’s", 
                  "ireland", "korea’s", "india’s", "mw", "ha", "km", "gw", "eu's", "srn", "inr", "ccc", "alberta", "wales", "welsh", "scottish", "australian", "japan's", "inuit", "www.gov.scot", "japan", "kingdom",
                  "scotland's", "gn", "scp", "ec", "pcf", "www.gov.uk", "gba", "gpg", "https", "net", "million")

combined_trigram <- combined_NDC %>% 
  unnest_tokens(trigram, text, token = "ngrams", n=3)

trigram_filtered <- combined_trigram %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word3 %in% stop_words$word) %>% 
  filter(!str_detect(word1, "\\d")) %>% 
  filter(!str_detect(word2, "\\d")) %>% 
  filter(!str_detect(word3, "\\d")) %>% 
  filter(!word1%in%remove_words) %>% 
  filter(!word2%in%remove_words) %>% 
  filter(!word3%in%remove_words)

trigram_filtered %>% 
  count(grouping, word1, word2, word3, sort = TRUE)

```
```{r}
trigram_united <- trigram_filtered %>% 
  unite(trigram, word1, word2, word3, sep=" ")

trigram_united %>% 
  count(grouping, trigram, sort=T)
```
```{r}
trigram_united%>% 
  count(grouping, trigram, sort=T) %>% 
  group_by(grouping) %>%
  slice_max(n, n=30) %>%
  ungroup() %>% 
  mutate(grouping=as.factor(grouping),
         trigram=reorder_within(trigram, n, grouping)) %>% 
  ggplot(aes(n, trigram,n, fill=grouping))+
  geom_col()+
  facet_wrap(~grouping, scales="free")+
  theme(legend.position = "none")+
  scale_y_reordered() +
  labs(y=NULL)
```

